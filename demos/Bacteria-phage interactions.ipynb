{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to match bacteria and phages via CRISPR spacers\n",
    "\n",
    "Many (probably most) CRISPR spacers found in bacterial genomes came from phages, viruses that infect bacteria. This notebook attempts to figure out which phages infect which bacteria in a metagenomic sample by matching CRISPR spacers to phages from a database. It uses [Crass](http://ctskennerton.github.io/crass/) to detect CRISPR spacers in metagenomic data, and BLAST to compare spacer sequences and metagenomic reads to databases of phage and bacterial genomes. \n",
    "\n",
    "### Notes on running this notebook\n",
    "\n",
    "* Commands in markdown (non-code) blocks that look `like this` are written in Bash. They can be run in a terminal (Linux / Mac) or in a Bash shell for Windows (Git Bash, for example). Alternatively, you can run those commands directly in this notebook by adding an exclamation point at the beginning of each line and changing the cell to a code block.\n",
    "* Many of the steps only need to be run once, even if you're analyzing multiple metagenomic datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install the necessary software\n",
    "\n",
    "\n",
    "We'll use [Crass](http://ctskennerton.github.io/crass/) to find CRISPR spacers in metagenomic data. The [Crass manual](http://ctskennerton.github.io/crass/assets/manual.pdf) has instructions for installation, but here's what worked on Ubuntu: \n",
    "\n",
    "* **Install Crass dependencies** \n",
    "\n",
    "```\n",
    "sudo apt-get install libxerces-c3-dev\n",
    "```\n",
    "\n",
    "```\n",
    "sudo add-apt-repository ppa:dns/gnu -y\n",
    "sudo apt-get update -q\n",
    "sudo apt-get install --only-upgrade autoconf\n",
    "```\n",
    "\n",
    "```\n",
    "sudo apt install libtool-bin\n",
    "```\n",
    "\n",
    "```\n",
    "wget http://www.zlib.net/zlib-1.2.11.tar.gz\n",
    "tar -xvzf zlib-1.2.11.tar.gz \n",
    "cd zlib-1.2.11/\n",
    "./configure --prefix=/usr/local/zlib\n",
    "make\n",
    "sudo make install\n",
    "```\n",
    "\n",
    "\n",
    "* **Install Crass: download tar from [here](http://ctskennerton.github.io/crass/)**\n",
    "\n",
    "```\n",
    "tar -xf crass-0.3.12.tar.gz \n",
    "cd crass-0.3.12/\n",
    "./autogen.sh \n",
    "./configure \n",
    "make \n",
    "sudo make install\n",
    "```\n",
    "\n",
    "* **Install BLAST locally**\n",
    "\n",
    "See the documentation [here](https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastDocs&DOC_TYPE=Download) to install BLAST to run on a local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download some microbiome data in fasta or fastq format\n",
    "\n",
    "We'll use a dataset from the Human Microbiome Project as an example. The file below is a metagenomic sample from subginvival plaque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Download and extract a dataset:\n",
    "```\n",
    "wget ftp://public-ftp.ihmpdcc.org/Illumina/subgingival_plaque/SRS014107.tar.bz2\n",
    "tar -xf SRS014107.tar.bz2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Download reference genomes and make BLAST databases\n",
    "\n",
    "We're using an NCBI phage database and the NCBI bacteria and archaea databases, accessible by downloading the accessions from [this FTP site](ftp://ftp.ncbi.nlm.nih.gov/genomes/GENOME_REPORTS/IDS/) using [`collect_accessions.py`](https://github.com/phageParser/phageParser/blob/master/parserscripts/collect_accessions.py) and then downloading genome sequences using [`acc2gb.py`](https://github.com/phageParser/phageParser/blob/master/parserscripts/acc2gb.py).\n",
    "\n",
    "This only needs to be done once, unless you want to periodically check for new sequences in NCBI and then recreate the databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Download accession numbers for phages, bacteria, and archaea:\n",
    "\n",
    "```\n",
    "python ../parserscripts/collect_accessions.py Phages.ids > phage_accessions.txt\n",
    "python ../parserscripts/collect_accessions.py Bactera.ids > bacteria_accessions.txt\n",
    "python ../parserscripts/collect_accessions.py Archaea.ids > archaea_accessions.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Download genome sequences - this takes a long time for bacteria (there are ~6700 sequences):\n",
    "\n",
    "```\n",
    "cat phage_accessions.txt | python parserscripts/acc2gb.py your@email.com nuccore fasta > phagegenomes.dat\n",
    "cat bacteria_accessions.txt | python parserscripts/acc2gb.py your@email.com nuccore fasta > bacteriagenomes.dat\n",
    "cat archaea_accessions.txt | python parserscripts/acc2gb.py your@email.com nuccore fasta > archaeagenomes.dat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create BLAST databases: one for bacteria and archaea, one for phages:\n",
    "\n",
    "```\n",
    "makeblastdb -in \"phagegenomes.dat\" -dbtype nucl -title phagedatabase_May2018 -out phagedb_May2018\n",
    "cat bacteriagenomes.dat archaeagenomes.dat | makeblastdb -in - -dbtype nucl -title bacdatabase_May2018 -out bacdb_May2018\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from creating a BLAST database should look something like this:\n",
    "\n",
    "```\n",
    "Building a new DB, current time: 05/11/2018 12:37:11\n",
    "New DB name:   /Documents/GitHub/phageParser/bacdb_May2018\n",
    "New DB title:  bacdatabase_May2018\n",
    "Sequence type: Nucleotide\n",
    "Keep Linkouts: T\n",
    "Keep MBits: T\n",
    "Maximum file size: 1000000000B\n",
    "Adding sequences from FASTA; added 6990 sequences in 103.433 seconds.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Crass\n",
    "\n",
    "The simplest syntax to run Crass on a fasta file is `crass input_fasta_file -o output_directory`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run Crass on one of the downloaded files:\n",
    "    \n",
    "```crass SRS014107.denovo_duplicates_marked.trimmed.1.fastq -o SRS014107```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Parse Crass output to get spacer sequences\n",
    "\n",
    "The Crass output we're interested in is the file `crass.crispr` that appears in the output directory. It's an XML file, so we can parse it. \n",
    "\n",
    "Setup:\n",
    "* create a folder called `spacers` in the Crass output folder\n",
    "* create a folder called `source_reads` in the Crass output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('/home/madeleine/crass-0.3.12/SRS014107/crass.crispr')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = \"/media/madeleine/My Passport/Data/Bac-phage-interactions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dictionary to store repeats and read headers to get them out of the original file later\n",
    "read_dict = {}\n",
    "\n",
    "# create a list of the accessions that come up\n",
    "read_dict_accessions = {}\n",
    "    \n",
    "for child in root: # each top-level child is a CRISPR array\n",
    "    repeat = child.attrib['drseq'] # the consensus repeat sequence identified by Crass\n",
    "    spacers = child[0][2] # the spacers associated with that repeat\n",
    "    source_reads = child[0][0] # the source reads that the spacers and repeats come from\n",
    "    \n",
    "    read_dict[repeat] = {}\n",
    "    \n",
    "    # create a file with the spacers\n",
    "    \n",
    "    with open(\"spacers/\" + repeat + \".fasta\", 'w') as spacer_file:\n",
    "        for spacer in spacers:\n",
    "            spacer_file.write('>' + spacer.attrib['spid'] + '\\n')\n",
    "            spacer_file.write(spacer.attrib['seq'] + '\\n')\n",
    "    \n",
    "    for read in source_reads:\n",
    "        header = read.attrib['accession']\n",
    "        read_dict[repeat][header] = []\n",
    "        read_dict_accessions[header] = repeat\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the sequences associated with each repeat for BLASTing\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for seq_record in SeqIO.parse(\"/home/madeleine/crass-0.3.12/SRS014107.denovo_duplicates_marked.trimmed.1.fastq\", \"fastq\"):\n",
    "    header = seq_record.id \n",
    "    if header in read_dict_accessions.keys():\n",
    "        read_dict[read_dict_accessions[header]][header] = seq_record.seq\n",
    "    #print(repr(seq_record.seq))\n",
    "    #print(len(seq_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save sequences to a file, one for each repeat\n",
    "\n",
    "for repeat in read_dict.keys():\n",
    "    with open(filepath + \"/source_reads/\" + repeat +'_reads' + \".fasta\", 'w') as repeat_file:\n",
    "        for header, sequence in read_dict[repeat].items():\n",
    "            repeat_file.write('>' + header + '\\n')\n",
    "            repeat_file.write(str(sequence) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: BLAST spacers against phage database, BLAST source reads against bacterial database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from Bio.Blast.Applications import NcbitblastnCommandline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loop through each file in the 'spacers' folder\n",
    "\n",
    "for filename in os.listdir(filepath + \"/spacers\"):\n",
    "    \n",
    "    # get repeat sequence from filename\n",
    "    repeat = filename.split('.')[0]\n",
    "    \n",
    "    # builds command line string\n",
    "    tblastn_obj = NcbitblastnCommandline(\n",
    "        cmd='blastn',\n",
    "        query=\"%s/spacers/%s\" %(filepath,filename),\n",
    "        db=\"phagedb_May2018\",\n",
    "        evalue=10,\n",
    "        outfmt=5,\n",
    "        out=\"%s/spacers_BLAST/%s.xml\" %(filepath,repeat)\n",
    "    )\n",
    "\n",
    "    tblastn_cline = str(tblastn_obj)\n",
    "\n",
    "    # executes command line string in bash\n",
    "    subprocess.call(tblastn_cline, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loop through each file in the source_reads folder\n",
    "\n",
    "for filename in os.listdir(filepath + \"/source_reads\"):\n",
    "    \n",
    "    # get repeat sequence from filename\n",
    "    repeat = filename.split('_')[0].split('.')[0]\n",
    "    \n",
    "    # builds command line string\n",
    "    tblastn_obj = NcbitblastnCommandline(\n",
    "        cmd='blastn',\n",
    "        query=\"%s/source_reads/%s\" %(filepath,filename),\n",
    "        db=\"bacdb_May2018\",\n",
    "        evalue=10,\n",
    "        outfmt=5,\n",
    "        out=\"%s/source_reads_BLAST/%s.xml\" %(filepath,repeat)\n",
    "    )\n",
    "\n",
    "    tblastn_cline = str(tblastn_obj)\n",
    "\n",
    "    # executes command line string in bash\n",
    "    subprocess.call(tblastn_cline, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Parse BLAST output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_blast(resultfile): #takes in the BLAST result, outputs list that can be made into csv\n",
    "    from Bio.Blast import NCBIXML\n",
    "    result_handle = open(resultfile)\n",
    "    blast_records = NCBIXML.parse(result_handle)\n",
    "    csv_list = []\n",
    "    \n",
    "    header = [  'Query',\n",
    "                'Name', 'Length', 'Score', 'Expect',\n",
    "                'QueryStart', 'QueryEnd',\n",
    "                'SubjectStart', 'SubjectEnd'\n",
    "            ]\n",
    "    \n",
    "    csv_list.append(header)\n",
    "    count = 0\n",
    "    for blast_record in blast_records:\n",
    "        '''help(blast_record.alignments[0].hsps[0])''' # these give help info for the parts \n",
    "        '''help(blast_record.alignments[0])        '''\n",
    "        count +=1\n",
    "        \n",
    "        query = blast_record.query\n",
    "        for alignment in blast_record.alignments:\n",
    "\n",
    "            name = alignment.title\n",
    "            length = alignment.length\n",
    "    \n",
    "            hsp = alignment.hsps[0] # I don't know if we will ever have more than one, so might as well take the first one.\n",
    "            score = hsp.score\n",
    "            expect = hsp.expect\n",
    "            if expect >= cutoff:\n",
    "                continue\n",
    "            querystart = hsp.query_start\n",
    "            queryend = hsp.query_end\n",
    "            subjectstart = hsp.sbjct_start\n",
    "            subjectend = hsp.sbjct_end\n",
    "            row = [query,name,length,score,expect,querystart,queryend,subjectstart,subjectend]\n",
    "            csv_list.append(row)\n",
    "            \n",
    "    result_handle.close()\n",
    "    return csv_list\n",
    "\n",
    "def write_csv(dest, csv_cont): #takes a list of lists object with each csv row as a list\n",
    "    with open(dest, 'w') as out_file:\n",
    "        writer= csv.writer(out_file, delimiter=',')\n",
    "        for row in csv_cont:\n",
    "            writer.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#source reads\n",
    "\n",
    "indir = filepath + \"/source_reads_BLAST\" \n",
    "outdir = filepath + \"/source_reads_BLAST_results\"\n",
    "\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "\n",
    "cutoff = 1\n",
    "\n",
    "for fn in os.listdir(\"%s/\" %indir):\n",
    "    ID = fn[:fn.index('.')]\n",
    "    if fn == 'sorted':\n",
    "        continue\n",
    "    csv_list = parse_blast(\"%s/%s\" %(indir,fn))\n",
    "    write_csv(\"%s/%s.csv\" %(outdir,ID), csv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spacers\n",
    "\n",
    "indir = filepath + \"/spacers_BLAST\" \n",
    "outdir = filepath + \"/spacers_BLAST_results\"\n",
    "\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "\n",
    "cutoff = 1\n",
    "\n",
    "for fn in os.listdir(\"%s/\" %indir):\n",
    "    ID = fn[:fn.index('.')]\n",
    "    if fn == 'sorted':\n",
    "        continue\n",
    "    csv_list = parse_blast(\"%s/%s\" %(indir,fn))\n",
    "    write_csv(\"%s/%s.csv\" %(outdir,ID), csv_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create interaction matrix"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:django]",
   "language": "python",
   "name": "conda-env-django-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
